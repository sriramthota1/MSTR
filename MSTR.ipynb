{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriramthota1/MSTR/blob/main/MSTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multilingual Speech Recognition Model for RAG**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LVVxQJoHLh52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3si1CvzLnEp",
        "outputId": "e8789988-61b8-4318-db3e-766ecbae5623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n",
            "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline, RagRetriever, RagTokenForGeneration, RagTokenizer\n",
        "from langdetect import detect, DetectorFactory\n",
        "import torch\n",
        "import os\n",
        "from googletrans import Translator\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def initialize_rag_model():\n",
        "    try:\n",
        "      rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
        "      retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
        "      rag_model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
        "      return retriever, rag_model, rag_tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing RAG model: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "retriever, rag_model, rag_tokenizer = initialize_rag_model()\n",
        "\n",
        "if not retriever or not rag_model or not rag_tokenizer:\n",
        "    raise RuntimeError(\"Failed to load RAG model and components.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "\n",
        "def speech_to_text_with_lang_detection(audio_file):\n",
        "    \"\"\"\n",
        "    Convert speech to text using Whisper model and detect the language.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(audio_file):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
        "\n",
        "    asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
        "    try:\n",
        "        result = asr(audio_file)\n",
        "        text = result['text']\n",
        "        detected_lang = detect(text)\n",
        "        return text, detected_lang\n",
        "    except Exception as e:\n",
        "        print(f\"Error in speech-to-text conversion: {e}\")\n",
        "        return \"Error during transcription\", \"unknown\""
      ],
      "metadata": {
        "id": "-zab6mo4ked5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translation** Function using Google Translate API"
      ],
      "metadata": {
        "id": "V0cGDjyclhDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text, src_lang, tgt_lang='en'):\n",
        "    \"\"\"\n",
        "    Translate text from source language to target language using Google Translate API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        translated = translator.translate(text, src=src_lang, dest=tgt_lang)\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return text"
      ],
      "metadata": {
        "id": "Dz9lu3C8keag"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Function**"
      ],
      "metadata": {
        "id": "BQR5-fG3lozZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_rag(question):\n",
        "    \"\"\"\n",
        "    Use RAG to generate an answer based on the input question.\n",
        "    \"\"\"\n",
        "    inputs = rag_tokenizer(question, return_tensors=\"pt\")\n",
        "    try:\n",
        "        print(f\"Input to RAG: {inputs}\")\n",
        "        with torch.no_grad():\n",
        "            outputs = rag_model.generate(**inputs)\n",
        "        print(f\"RAG outputs: {outputs}\")\n",
        "        answer = rag_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"RAG generation error: {e}\")\n",
        "        return \"Error generating answer with RAG.\""
      ],
      "metadata": {
        "id": "y-OVXiFpkeXh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Audio Query Function**"
      ],
      "metadata": {
        "id": "TcQ8Syu5ltAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_query(audio_file, target_lang='en'):\n",
        "    \"\"\"\n",
        "    Process an audio file by detecting the language, transcribing it, translating it, and using RAG to generate an answer.\n",
        "    \"\"\"\n",
        "\n",
        "    text, detected_lang = speech_to_text_with_lang_detection(audio_file)\n",
        "    print(f\"Detected language: {detected_lang}\")\n",
        "    print(f\"Transcribed text: {text}\")\n",
        "\n",
        "\n",
        "    if detected_lang != target_lang and detected_lang != 'unknown':\n",
        "        translated_text = translate_text(text, src_lang=detected_lang, tgt_lang=target_lang)\n",
        "    else:\n",
        "        translated_text = text\n",
        "\n",
        "\n",
        "    answer = generate_answer_with_rag(translated_text)\n",
        "\n",
        "    if answer and target_lang != 'en':\n",
        "        rag_answer_translated = translate_text(answer, src_lang='en', tgt_lang=target_lang)\n",
        "    else:\n",
        "        rag_answer_translated = answer\n",
        "\n",
        "    return (f\"Detected Language: {detected_lang}\\n\\n\"\n",
        "            f\"Transcription: {text}\\n\\n\"\n",
        "            f\"Transcription (translated to {target_lang}): {translated_text}\\n\\n\"\n",
        "            f\"RAG Answer: {answer}\\n\\n\"\n",
        "            f\"RAG Answer (translated to {target_lang}): {rag_answer_translated}\")\n"
      ],
      "metadata": {
        "id": "Jhls8FoSkeVY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio Interface**"
      ],
      "metadata": {
        "id": "MlCX_Cy5l2Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_interface(audio, target_lang='en'):\n",
        "    return process_audio_query(audio, target_lang)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[gr.Audio(type=\"filepath\"), gr.Textbox(label=\"Target Language (e.g., 'en', 'es', 'te')\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"Multilingual Speech Recognition Model for RAG\",\n",
        "    description=\"Upload an audio file, select the target language, and the system will automatically detect the language, transcribe it, translate it, and use RAG to generate an answer based on the input text.\"\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "UQYg8MY5keSc",
        "outputId": "4d65f7e7-7350-41ec-c8ec-0175f9eb1526"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e1e3a8cff350b926a7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e1e3a8cff350b926a7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}